# Lab 2
Вся работа выполнена в ноутбуке 2_lab.ipynb

## Датасет
https://github.com/AlexKly/Detailed-NER-Dataset-RU

## Модели
### Rule-based
В качестве rule-based решения использовали Natasha
### Базовая
https://huggingface.co/dslim/bert-base-NER
### Обученная
https://disk.yandex.ru/d/_EBKKsESs6RMHg (я честно хотел загрузить на huggingface, но не смог)

## Train log

| Step | Training Loss |
| ----- | ----- | 
| 500 | 0.331800 |
| 1000 | 0.157500 |
| 1500 | 0.101300 |
| 2000 | 0.067800 |
| 2500 | 0.045900 |
| 3000 | 0.031100 |
| 3500 | 0.023900 |
| 4000 | 0.019700 |

## Метрики
| Model | Accuracy, % | Accuracy no "O", % |
| ----- | ----- | ----- |
| Natasha | 89.46 | 26.32 |
| BERT base NER | 89.34 | 8.68 |
| BERT base NER SFT | 86.28 | 30.29 |

## Выводы
* Хорошо обученные нейросетевые решения, в частности трансформеры лучше rule-based решений
* Необходима тренировка на релевантных данных (в первую очередь язык), модель натренированная на нерелевантных данных хуже rule-based решения
* Encoder-only трансформеры показывают себя неплохо, имеет смысл попробовать генеративные encoder-decoder и decoder-only
* Кажется, что accuracy по классам кроме "O" более репрезентативна

## Возможности для улучшения
* Использования bert large вместо base
* Претрейн модели на русском языке, либо файнтюн русскоязычного не ner bert
* Использовать roberta или encoder-decoder, в частности t5